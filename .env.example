# ==============================================================================
# OrtoKompanion Environment Configuration
# ==============================================================================
# Copy this file to .env.local and fill in your values
# Never commit .env.local to version control!

# ==============================================================================
# REQUIRED - OpenAI API Configuration
# ==============================================================================

# Your OpenAI API key (REQUIRED for AI features)
# Get it from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your_api_key_here

# ==============================================================================
# App Configuration
# ==============================================================================

NEXT_PUBLIC_APP_NAME=Ortokompanion
NEXT_PUBLIC_APP_VERSION=1.0.0

# ==============================================================================
# OPTIONAL - AI Service Configuration
# ==============================================================================

# AI model to use (default: gpt-4o-mini)
# Options: gpt-4o-mini, gpt-4o, gpt-4-turbo
# AI_MODEL=gpt-4o-mini

# Default temperature for AI responses (0-2, default: 0.7)
# Lower = more focused/deterministic, Higher = more creative
# AI_TEMPERATURE=0.7

# Maximum tokens per AI response (default: 1000)
# AI_MAX_TOKENS=1000

# Request timeout in milliseconds (default: 30000 = 30s)
# AI_TIMEOUT=30000

# Maximum retry attempts for failed requests (default: 2)
# AI_MAX_RETRIES=2

# ==============================================================================
# OPTIONAL - Cache Configuration
# ==============================================================================

# Maximum entries in memory cache (default: 100)
# CACHE_MAX_SIZE=100

# Default TTL for cache entries in seconds (default: 3600 = 1 hour)
# CACHE_DEFAULT_TTL=3600

# Explanation cache TTL in seconds (default: 86400 = 24 hours)
# CACHE_EXPLANATION_TTL=86400

# Study plan cache TTL in seconds (default: 3600 = 1 hour)
# CACHE_STUDY_PLAN_TTL=3600

# Content recommendation cache TTL in seconds (default: 1800 = 30 min)
# CACHE_CONTENT_REC_TTL=1800

# ==============================================================================
# OPTIONAL - Rate Limiting Configuration
# ==============================================================================

# Rate limit for AI generation endpoint (requests per minute, default: 60)
# RATE_LIMIT_AI_STANDARD=60

# Rate limit for AI chat endpoint (requests per minute, default: 100)
# RATE_LIMIT_AI_CHAT=100

# Rate limit for strict/expensive operations (requests per minute, default: 20)
# RATE_LIMIT_AI_STRICT=20

# Rate limit for anonymous/unauthenticated requests (requests per minute, default: 10)
# RATE_LIMIT_ANONYMOUS=10

# ==============================================================================
# OPTIONAL - Feature Flags
# ==============================================================================

# Enable AI-powered features (default: true)
# ENABLE_AI_FEATURES=true

# Enable caching (default: true)
# ENABLE_CACHE=true

# Enable rate limiting (default: true)
# ENABLE_RATE_LIMIT=true

# Enable detailed logging (default: false in production)
# ENABLE_DETAILED_LOGS=false

# ==============================================================================
# OPTIONAL - External Services (Future)
# ==============================================================================

# Redis URL for distributed caching (optional, for production scaling)
# REDIS_URL=redis://localhost:6379

# Sentry DSN for error tracking (optional)
# SENTRY_DSN=https://...

# Analytics tracking ID (optional)
# ANALYTICS_ID=...

# ==============================================================================
# DEVELOPMENT ONLY
# ==============================================================================

# Next.js development mode
# NODE_ENV=development

# Port for development server (default: 3000)
# PORT=3000
